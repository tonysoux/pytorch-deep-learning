{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_pytorch_computer_vision_exercises.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonysoux/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/).\n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA).\n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ],
      "metadata": {
        "id": "Vex99np2wFVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "598d67bf-e01c-46e2-b221-13bf7d673a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 19 09:23:59 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "179416c1-0daa-4732-ca5b-0feafb147e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ],
      "metadata": {
        "id": "FSFX7tc1w-en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Autonomous driving, control quality and"
      ],
      "metadata": {
        "id": "VyWRkvWGbCXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find."
      ],
      "metadata": {
        "id": "oBK-WI6YxDYa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1rxD6GObCqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each.\n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ],
      "metadata": {
        "id": "XeYFEqw8xK26"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ocvOdWKcbEKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ],
      "metadata": {
        "id": "DKdEEFEqxM-8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ],
      "metadata": {
        "id": "lvf-3pODxXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "train_data=MNIST(\"data/mnist\", train = True, transform=transforms.ToTensor(), download=True)\n",
        "print(train_data[5000][0].shape)\n",
        "\n",
        "test_data=MNIST(\"data/mnist\", train = False, transform=transforms.ToTensor(), download=True)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "SHjeuN81bHza",
        "outputId": "ce0cd63c-f4cb-4eb4-c794-1225498961e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: data/mnist\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6AnImvLXAv4V",
        "outputId": "250e8338-de1c-45fe-d8d7-9413e38e9ac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ],
      "metadata": {
        "id": "qxZW-uAbxe_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(42)\n",
        "plt.figure()\n",
        "for i in range(5):\n",
        "  random_idx=torch.randint(0, len(train_data), size=[1]).item()\n",
        "  print(random_idx)\n",
        "  plt.subplot(1,5,i+1)\n",
        "  print(train_data[random_idx][0].shape)\n",
        "  plt.imshow(train_data[random_idx][0].squeeze(), cmap ='gray')\n",
        "  plt.axis(False)\n"
      ],
      "metadata": {
        "id": "QVFsYi1PbItE",
        "outputId": "da8760ee-8335-47b6-dddd-3b153b2808ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31378\n",
            "torch.Size([1, 28, 28])\n",
            "29014\n",
            "torch.Size([1, 28, 28])\n",
            "47210\n",
            "torch.Size([1, 28, 28])\n",
            "18954\n",
            "torch.Size([1, 28, 28])\n",
            "18231\n",
            "torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ],
      "metadata": {
        "id": "JAPDzW0wxhi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size=32\n",
        "train_dataloader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader=DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(next(iter(train_dataloader))[0].shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "ALA6MPcFbJXQ",
        "outputId": "bd1210b1-0e32-4297-8920-f938aec547d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ],
      "metadata": {
        "id": "bCCVfXk5xjYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our input data.\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ],
      "metadata": {
        "id": "sf_3zUr7xlhy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ],
      "metadata": {
        "id": "w1CsHhPpxp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ],
      "metadata": {
        "id": "qQwzqlBWxrpG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ],
      "metadata": {
        "id": "lj6bDhoWxt2y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset.\n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been.\n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error?\n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ],
      "metadata": {
        "id": "VHS20cNTxwSi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}